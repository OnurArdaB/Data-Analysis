# -*- coding: utf-8 -*-
"""BreastCancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fBJzfyIWBnZ5VdOp8mlbnKH0rnL2sWeK

Breast Cancer Analysis
"""

from google.colab import drive
drive.mount("gdrive")

"""###Import necessary libraries."""

import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import tensorflow as tf

"""###Read the dataset."""

df = pd.read_csv("/content/gdrive/My Drive/DataAnalysis/BreastCancerWisconsin/data.csv")

df.head(-1)

"""## Metadata regarding on the dataset.

Dataset taken from:<a href="https://www.kaggle.com/uciml/breast-cancer-wisconsin-data">Kaggle</a>

Attribute Information:

1) ID number
2) Diagnosis (M = malignant, B = benign)
3-32)

Ten real-valued features are computed for each cell nucleus:

  * radius (mean of distances from center to points on the perimeter)
  * texture (standard deviation of gray-scale values)
  * perimeter
  * area
  * smoothness (local variation in radius lengths)
  * compactness (perimeter^2 / area - 1.0)
  * concavity (severity of concave portions of the contour)
  * concave points (number of concave portions of the contour)
  * symmetry
  * fractal dimension ("coastline approximation" - 1)

The mean, standard error and "worst" or largest (mean of the three
largest values) of these features were computed for each image,
resulting in 30 features. For instance, field 3 is Mean Radius, field
13 is Radius SE, field 23 is Worst Radius.

All feature values are recoded with four significant digits.

Missing attribute values: none

Class distribution: 357 benign, 212 malignant
"""

df.describe()

df.info()

"""Let's check for nan values."""

for col in df.columns:
  print(col,":",df[col].isnull().sum())

"""Let's drop Unnamed: 32 before we forget.
id field is also unnecessary.
"""

df.drop(columns=["Unnamed: 32","id"],inplace=True)
df.columns

"""## Visualisation

Most of the EDA graphs are taken from:<a href="https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization">Here!</a>

### Target class distribution.
"""

sns.countplot(df.diagnosis)
B,M = df.diagnosis.value_counts()
plt.title("Malignant:"+str(M)+ " vs "+"Benign:"+str(B))
plt.show()

data_ = df.diagnosis
data = df.drop(columns=["diagnosis"],inplace=False)
data_n_2 = (data - data.mean()) / (data.std())              #
data = pd.concat([data_,data_n_2],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(30,10))
sns.violinplot(x="features", y="value", hue="diagnosis", data=data,split=True, inner="quart")
plt.xticks(rotation=90)
plt.show()

sns.set(style="whitegrid", palette="muted")
data_ = df.diagnosis
data = df.drop(columns=["diagnosis"],inplace=False)
data_n_2 = (data - data.mean()) / (data.std())              
data = pd.concat([data_,data_n_2],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(30,10))
sns.swarmplot(x="features", y="value", hue="diagnosis", data=data)

plt.xticks(rotation=90)
plt.show()

fig, ax = plt.subplots(figsize=(15,12))         # Sample figsize in inches
sns.heatmap(df.corr(), annot=True, linewidths=.5,ax=ax, fmt= '.1f',mask= np.triu(np.ones_like(df.corr(), dtype=np.bool)))

"""#Data Preprocessing

* Normalization or Stadardization
"""

df.dtypes

df.describe()

from sklearn.preprocessing import StandardScaler,MinMaxScaler
target = df["diagnosis"]
features = df.drop(columns=["diagnosis"])
norm = MinMaxScaler()
features = pd.DataFrame(norm.fit_transform(features),columns=features.columns)
features.head()

target_dict = {
    "M":1,
    "B":0
}
target = target.map(target_dict)

"""#ANN"""

X_train,X_test,y_train,y_test = train_test_split(features,target,test_size=0.33,random_state=42)

model = tf.keras.Sequential()

model.add(tf.keras.layers.Flatten(input_shape=(30,)))

model.add(tf.keras.layers.Dense(16, activation='swish'))

model.add(tf.keras.layers.Dense(16, activation='swish'))

model.add(tf.keras.layers.Dense(16, activation='swish'))

model.add(tf.keras.layers.Dense(16, activation='swish'))

model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

model.compile(loss="binary_crossentropy",optimizer="adam",metrics=['accuracy'])

X_train.shape

model.fit(X_train,y_train,epochs=30,batch_size=1)

print(classification_report(y_test,model.predict_classes(X_test)))

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, model.predict_classes(X_test))
sns.heatmap(cm,annot=True)