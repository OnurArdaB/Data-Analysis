# -*- coding: utf-8 -*-
"""AdultIncome.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19xSf71EQbi4Ya62QJAiFKp30VEnmMFib

# Adult Income Dataset
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Data Analysis and Pre-processing

### Import necessary libraries.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline
# %autocall 2
from xgboost import XGBClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import OneHotEncoder,LabelEncoder
from sklearn.metrics import classification_report

"""There are few things to consider before starting to work.First,the dataset is already splitted to train and test for model building operations.Secondly,this dataset contains data suited for classification operations."""

df_train = pd.read_csv("/content/drive/My Drive/DataAnalysis/AdultIncome/adult.data",sep=',',names=["age","workclass","fnlwgt","education","education-num","marital-status","occupation","relationship","race","sex","capital-gain","capital-loss","hours-per-week","native-country","<=50K"],na_values=[" ? ","? "," ?"],na_filter=True)
df_test = pd.read_csv("/content/drive/My Drive/DataAnalysis/AdultIncome/adult.test",sep=',',names=["age","workclass","fnlwgt","education","education-num","marital-status","occupation","relationship","race","sex","capital-gain","capital-loss","hours-per-week","native-country","<=50K"],na_values=[" ? ","? "," ?"],na_filter=True)
df = df_train.append(df_test)
df["<=50K"].replace(regex=True,inplace=True,to_replace=r'\.',value=r'')

"""Let's check what we have imported."""

df.head(-1)

"""### Information regarding on the dataset"""

df.shape

"""There is a large amount of data.We are going to make a binary classification.We will try to investigate whether an individual will earn more than 50 K or not regarding to the information given to us."""

df.info()

df.describe()

df.isnull().sum()

"""Well values are matching so we can continue to decide what to do on the missing values."""

for column in df:
  print(column," : ",df[column].unique())
  print("------------------------------------------------------------------------------------------------------------------")

"""### Exploratory Data Analysis - (EDA)"""

sns.pairplot(df)

"""#### Age Distribution"""

plt.figure(figsize=(20,5))
plt.xticks(df.age.unique())     
plt.bar(df.age.unique(),df.age.value_counts())

"""It is more common to observe people aged 60 and lower in this dataset.Despite the fact that some ages between 17 to 60 seems low in the dataset,dramatic decrease occurs after age 60.

* Age is right skewed.
* Ranges from 17 to 90 having 89 as the only missing age in the range.
* 59 is the break point for the major decrease in the age.

#### Education
"""

plt.figure(figsize=(20,10))
plt.bar(df.education.unique(),df.education.value_counts())

"""* Bachelors is the most common education degree in the dataset followed by HS-grad and 11th.
* It can be observed that Masters is also the 4th most frequent education class label and Doctorate is one of the lowest frequent class label in the dataset.
* Some assumptions can be made:
  * People tends to finish the high school;HS-Grad is the second frequent class and 12th grade is the least freq. class label.

#### Marital Status
"""

plt.figure(figsize=(20,5))
plt.bar(df["marital-status"].unique(),df["marital-status"].value_counts())

"""Despite the fact that it seems dataset contains people that is never married,this is due to the fact that young people takes a great amount of part in the dataset."""

plt.figure(figsize=(20,5))
mask = df[df.age<30]
plt.bar(mask["marital-status"].unique(),mask["marital-status"].value_counts())

plt.figure(figsize=(20,5))
mask = df[(df.age>30) & (df.age<50)]
plt.bar(mask["marital-status"].unique(),mask["marital-status"].value_counts())

plt.figure(figsize=(20,5))
mask = df[(df.age>50)]
plt.bar(mask["marital-status"].unique(),mask["marital-status"].value_counts())

"""People above 50 is more likely to be married or separated while people below 50 tends to be never married or divorced.Let's check which ages tends to be never married more.

####Relationship
"""

plt.figure(figsize=(20,5))
plt.bar(df.relationship.unique(),df.relationship.value_counts())

"""####Race"""

plt.figure(figsize=(20,5))
plt.bar(df.race.unique(),df.race.value_counts())

"""Race class is imbalanced,most of the instances are white followed by black having a large difference."""

plt.bar(df.sex.unique(),df.sex.value_counts())

plt.bar(df["<=50K"].unique(),df["<=50K"].value_counts())

fig,axs = plt.subplots(2)
mask=df[df["sex"]==" Female"]
axs[0].set_title("Female Income")
axs[0].bar(mask["<=50K"].unique(),mask["<=50K"].value_counts())
mask=df[df["sex"]==" Male"]     
axs[1].set_title("Male Income")
axs[1].bar(mask["<=50K"].unique(),mask["<=50K"].value_counts())

mask = df[(df["<=50K"]==" >50K") & (df["sex"]==" Female")]
fig,axs = plt.subplots(2,figsize=(20,10))
mask.head()
axs[0].set_title("Education freq. regarding to Income:Female")
axs[0].bar(mask.education.unique(),mask.education.value_counts())
mask = df[(df["<=50K"]==" >50K") & (df["sex"]==" Male")]
axs[1].set_title("Education freq. regarding to Income:Male")
axs[1].bar(mask.education.unique(),mask.education.value_counts())

mask = df[(df["<=50K"]==" >50K") & (df["sex"]==" Female")]
fig,axs = plt.subplots(2,figsize=(20,10))
mask.head()
axs[0].set_xticks(mask.age.unique())
axs[0].set_title("Age freq. regarding to Income:Female")
axs[0].bar(mask.age.unique(),mask.age.value_counts())

mask = df[(df["<=50K"]==" >50K") & (df["sex"]==" Male")]
axs[1].set_xticks(mask.age.unique())
axs[1].set_title("Age freq. regarding to Income:Male")
axs[1].bar(mask.age.unique(),mask.age.value_counts())

"""###Pre-Processing & Encoding

There were lots of missing values exists in the dataset.After certain operations we have seen that almost any imputation method results same as dropping the missing values.
"""

df_temp = df.dropna()

"""Let's check for the NaN after the drop method just to be sure."""

for element in df_temp.isnull().sum():
  if(element==True):
      print("NaN found..")

"""Now we are going to label some columns as categorical in order to encode easier."""

df_temp["sex"] = df_temp["sex"].astype('category')
df_temp["workclass"] = df_temp["workclass"].astype('category')
df_temp["marital-status"] = df_temp["marital-status"].astype('category')
df_temp["occupation"] = df_temp["occupation"].astype('category')
df_temp["race"] = df_temp["race"].astype('category')
df_temp["relationship"] = df_temp["relationship"].astype('category')
df_temp["native-country"] = df_temp["native-country"].astype('category')

"""We separate the data to features and target data.Finally we are One-Hot Encoding several categorical classes."""

target = df_temp["<=50K"]
df_temp = df_temp.drop(columns=['<=50K'])
df_temp = pd.get_dummies(df_temp)

"""Let's see what have happened after One-Hot Encoding"""

df_temp.head()

"""Now we are going to label-encode several columns and split the encoded data to train and test data."""

from sklearn.model_selection import train_test_split
from collections import defaultdict
d = defaultdict(LabelEncoder)
df_temp = df_temp.apply(lambda x:d[x.name].fit_transform(x))
X_train, X_test, y_train, y_test = train_test_split(df_temp,target,test_size=0.33, random_state=42)

"""We still have some some data that needs to be scaled.Scaling allows most of the ML models to show better performance.In our case we are using ANN and we do not wan't any feature to show more affect than others."""

from sklearn.preprocessing import StandardScaler
X_train = StandardScaler().fit_transform(X_train)

x_test = StandardScaler().fit_transform(X_test)

"""###Artificial Neural Network"""

import tensorflow as tf

model = tf.keras.Sequential()

model.add(tf.keras.layers.Flatten(input_shape=(104,)))

model.add(tf.keras.layers.Dense(16, activation='swish'))

model.add(tf.keras.layers.Dense(16, activation='relu'))

model.add(tf.keras.layers.Dense(16, activation='relu'))

model.add(tf.keras.layers.Dense(16, activation='swish'))

model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

model.compile(loss="binary_crossentropy",optimizer="adam",metrics=['accuracy'])

dfcurr=pd.DataFrame(X_train)
for col in dfcurr.columns:
  dfcurr[col]=dfcurr[col].astype(int)

y_train = np.ravel(y_train)

coder = LabelEncoder().fit_transform(y_train)

print(coder)

model.fit(X_train,coder,epochs=25,batch_size=1,verbose=1)

y_test = np.ravel(y_test)

score = model.evaluate(x_test,LabelEncoder().fit_transform(y_test))

y_true,y_pred = LabelEncoder().fit_transform(y_test),model.predict(x_test).round()
print(classification_report(y_true,y_pred))

"""Model works good when it tries to identify <=50K which is an expected result since that class is oversampled than the second class.The model also shows a good precision on second class but fails to show a good recall."""