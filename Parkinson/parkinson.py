# -*- coding: utf-8 -*-
"""Parkinson.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qmxbv6U0brWiFEvksVYv3ToZMt4WWw39
"""

from google.colab import drive
drive.mount("gdrive")

"""Import necesaary libraries."""

import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#Machine Learning Libraries
from sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.model_selection import train_test_split
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler,StandardScaler

"""Read the dataset into a dataframe."""

df=pd.read_csv("/content/gdrive/My Drive/DataAnalysis/Parkinson/parkinsons.data")

df.head()

"""##Metadata regarding on the dataset."""

df.describe()

"""This dataset can be used for building a model in order to make classification of healthy and PD."""

df.info()

"""Only *status* field is a categorical variable and other columns are continious variables."""

df.isnull().sum()

"""There is no empty field in the dataset which is a good thing because we can get started on analyzing the dataset.

## EDA

###Target class distributions.
"""

sns.countplot(df.status)
PD,H = df.status.value_counts()
plt.title("Healthy:"+str(H)+ " vs "+"PD:"+str(PD))
plt.show()

"""###Violinplot"""

data_ = df.status
data = df.drop(columns=["status","name"],inplace=False)
data_n_2 = (data - data.mean()) / (data.std())              
data = pd.concat([data_,data_n_2],axis=1)
data = pd.melt(data,id_vars="status",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(30,10))
sns.violinplot(x="features", y="value", hue="status", data=data,split=True, inner="quart")
plt.xticks(rotation=90)
plt.show()

"""###Swarmplot
* Since there is not many instance of data we can get a good view from this plot.
"""

sns.set(style="whitegrid", palette="muted")
data_ = df.status
data = df.drop(columns=["status","name"],inplace=False)
data_n_2 = (data - data.mean()) / (data.std())              
data = pd.concat([data_,data_n_2],axis=1)
data = pd.melt(data,id_vars="status",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(30,10))
sns.swarmplot(x="features", y="value", hue="status", data=data)

plt.xticks(rotation=90)
plt.show()

"""The swarmplot shows that there exists some noisy points which could cause our model to work wrongly and in order to solve this feature engineering might be necessary.

###Correlation Matrix Using Heatmap
* Before moving on working with models we can use a correlation matrix since some models such as Naive Bayes can get affected with related features.
* If there exist a linear relation between some features correlation matrix will help us to identify those features.
"""

fig, ax = plt.subplots(figsize=(15,12))         # Sample figsize in inches
sns.heatmap(df.drop(columns=["name"]).corr(), annot=True, linewidths=.5,ax=ax, fmt= '.1f',mask= np.triu(np.ones_like(df.drop(columns=["name"]).corr(), dtype=np.bool)))

"""##Data Preprocessing"""

features = df.drop(columns=["status","name"])
target = pd.DataFrame(df["status"],columns=["status"])

target.head()

features.head()

features.describe()

"""Standard Scaling (Standardisation) is useful when there is a gaussian dist. in the dataset fields and when our algorithm makes assumptions regarding on the dataset.

Normalizing the data is sensitive to outliers, so if there are outliers in the data set it is a bad practice. Standardization creates a new data not bounded (unlike normalization).
"""

features = pd.DataFrame(StandardScaler().fit_transform(features),columns=features.columns)

features.head()

features.describe()

X_train,X_test,y_train,y_test = train_test_split(features,target,test_size=0.33,random_state=42)

X_train.shape

"""##Model Selection
* Naive Bayes
* Logistic Regression
* Decision Tree
* Random Forest
* Gradient Boosted Tree
* SVM
* ANN
"""

from sklearn.model_selection import GridSearchCV

"""#### Gaussian NB"""

gnb = GaussianNB()
gnb.fit(X_train,y_train)
print(classification_report(y_test,gnb.predict(X_test))

"""#### Logistic Regression"""

grid={"C":np.logspace(-3,3,7), "penalty":["l1","l2"]}# l1 lasso l2 ridge
lr = LogisticRegression(verbose=False)
lr_cv=GridSearchCV(lr,grid,cv=10,verbose=False)
lr_cv.fit(X_train,y_train)

print(classification_report(y_test,lr_cv.predict(X_test)))

print("Best Parameters:",lr_cv.best_params_)
print("Best Score:",lr_cv.best_score_)

"""#### Decision Tree"""

dt = DecisionTreeClassifier()
grid={
    'criterion':['gini','entropy'],
    'max_depth':range(1,10),
    'min_samples_split':range(2,5),
     'min_samples_leaf':range(2,5), 
      'max_leaf_nodes':range(2,10)
}

dt_cv = GridSearchCV(dt,grid,verbose=False)
dt_cv.fit(X_train,y_train)

print(classification_report(y_test,dt_cv.predict(X_test)))

print("Best Parameters:",dt_cv.best_params_)
print("Best Score:",dt_cv.best_score_)

"""#### Random Forest"""

rf = RandomForestClassifier(n_estimators=200)
rf.fit(X_train,np.ravel(y_train))

print(classification_report(y_test,rf.predict(X_test)))

"""#### XGBoost"""

xgb = XGBClassifier(n_estimators=200)
xgb.fit(X_train,y_train)
print(classification_report(y_test,xgb.predict(X_test)))

tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                     'C': [1, 10, 100, 1000]},
                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]
svm_cv = GridSearchCV(
        SVC(), tuned_parameters, scoring='precision'
    )
svm_cv.fit(X_train, np.ravel(y_train))

print(classification_report(y_test,svm_cv.predict(X_test)))

model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(22,)))
model.add(tf.keras.layers.Dense(16,activation="swish"))
model.add(tf.keras.layers.Dense(16,activation="swish"))
model.add(tf.keras.layers.Dense(8,activation="swish"))
model.add(tf.keras.layers.Dense(1,activation="sigmoid"))
model.compile(loss="binary_crossentropy",optimizer="adam",metrics=['accuracy'])

model.fit(X_train,y_train,epochs=30,batch_size=1)

print(classification_report(y_test,model.predict(X_test).round()))

"""Among all these models ANN showed a very nice performance hence it can be used as a PD classifier but must be tested with more data when it is possible before used as a product."""