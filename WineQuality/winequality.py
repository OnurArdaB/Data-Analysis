# -*- coding: utf-8 -*-
"""WineQuality.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z5I3TGE1BmLRFu3OhcdMiUB11vnh2nqC

# Wine Quality Dataset Analysis

Dataset is provided from; https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Importing necessary libraries"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.metrics import accuracy_score,classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

from xgboost import XGBClassifier

"""## Extract the dataset to pandas dataframe"""

df_red = pd.read_csv("/content/drive/My Drive/DataAnalysis/WineQuality/winequality-red.csv",sep=";")
df_white = pd.read_csv("/content/drive/My Drive/DataAnalysis/WineQuality/winequality-white.csv",sep=";")

"""Let's check what we have imported."""

df_red.head(-1)

df_white.head(-1)

"""## Information regarding to the datasets"""

print("Column names for red wine:",list(df_red.columns))
print("Column names for white wine:",list(df_white.columns))

df_red.describe()

df_white.describe()

df_red.info()

df_white.info()

"""## Check NaN in the datasets"""

for column in df_red.columns:
  print("Column name:",column, "\nCount of NaN values:",df_red[column].isnull().sum(),"\n")

print("----------------------------------")

for column in df_white.columns:
  print("Column name:",column, "\nCount of NaN values:",df_white[column].isnull().sum(),"\n")

"""There is no NaN value in the datasets."""

for column in df_red.columns:
  print("Column:",column)
  print("Unique Values:",df_red[column].unique())

for column in df_red.columns:
  print("Column:",column)
  print("Unique Values:",df_white[column].unique())

"""## Exploratory Data Analysis - (EDA)

### Red Wine
"""

plt.figure(figsize=(12,6))
mask = np.triu(np.ones_like(df_red.corr(), dtype=np.bool))
sns.heatmap(df_red.corr(),annot=True,mask=mask,linewidths=0, vmin=-1, cmap="RdBu_r")

sns.pairplot(df_red,hue="quality")

plt.bar(df_red["quality"].unique(),df_red["quality"].value_counts())

"""#### Sub-Classing (Bad,Medium,Good)"""

rating = ['bad','medium','good']
df_new_red = df_red.copy()
for index, row in df_new_red.iterrows():
  if(row.quality < 6):
    row = rating[0]
  elif(row.quality == 6 ):
    row = rating[1]
  else:
    row = rating[2]
  df_new_red.at[index,"rating"] = row
df_new_red.drop(["quality"],axis=1,inplace=True)

plt.bar(df_new_red["rating"].unique(),df_new_red["rating"].value_counts())

"""#### Sub-Classing (Bad,Good)"""

rating = ['bad','good']
df_bin_red = df_red.copy()
for index, row in df_bin_red.iterrows():
  if(row.quality <= 6):
    row = rating[0]
  else:
    row = rating[1]
  df_bin_red.at[index,"rating"] = row
df_bin_red.drop(["quality"],axis=1,inplace=True)

plt.bar(df_bin_red["rating"].unique(),df_bin_red["rating"].value_counts())

"""### White Wine"""

plt.figure(figsize=(12,6))
mask = np.triu(np.ones_like(df_white.corr(), dtype=np.bool))
sns.heatmap(df_white.corr(),annot=True,mask=mask,linewidths=0, vmin=-1, cmap="RdBu_r")

sns.pairplot(df_white,hue="quality")

plt.bar(df_white["quality"].unique(),df_white["quality"].value_counts())

"""#### Sub-Classing (Bad,Medium,Good)"""

rating = ['bad','medium','good']
df_new_white = df_white.copy()
for index, row in df_new_white.iterrows():
  if(row.quality < 6):
    row = rating[0]
  elif(row.quality == 6 ):
    row = rating[1]
  else:
    row = rating[2]
  df_new_white.at[index,"rating"] = row
df_new_white.drop(["quality"],axis=1,inplace=True)

plt.bar(df_new_white["rating"].unique(),df_new_white["rating"].value_counts())

"""#### Sub-Classing (Bad,Good)"""

rating = ['bad','good']
df_bin_white = df_white.copy()
for index, row in df_bin_white.iterrows():
  if(row.quality <= 6):
    row = rating[0]
  else:
    row = rating[1]
  df_bin_white.at[index,"rating"] = row
df_bin_white.drop(["quality"],axis=1,inplace=True)

plt.bar(df_bin_white["rating"].unique(),df_bin_white["rating"].value_counts())

"""# Model Selection&Build

We will try to evaluate models with regarding to three sequences.First sequence will contain all of the quality measures from 3 to 8 for red wine and 3 to 9 for white wine.

Second sequence will contain only three quality measures, bad,medium,good.

For the final sequence ,we will try to evaluate a model for binary classification.Quality measures stated as good or bad.

* Gaussian Naive Bayes
* Logistic Regression
* SVM
* Random Forest Classifier
* XGBoost Classifier

## Red Wine

### Without Sub-Classification

Train,test splitting using test size = 0.2 and random state as 42
"""

train_red_X, test_red_X, train_red_y, test_red_y = train_test_split(df_red.drop(["quality"],axis=1,inplace=False),df_red["quality"],test_size=0.2,random_state=42)

"""We are using StandardScaler for scaling.Values would range from -1 to 1."""

scalar=StandardScaler()
X_train=scalar.fit_transform(train_red_X,train_red_y)
X_test=scalar.fit_transform(test_red_X,test_red_y)

"""For the first model we choose Gaussian Naive Bayes.There exists only 2 hyperparameters for Naive Bayes smoothing and priors.We will not use grid search cross validation on Naive Bayes."""

GNB = GaussianNB()
GNB.fit(X_train,train_red_y)
print("Score:",GNB.score(X_test,test_red_y))
print("Class priors:",GNB.class_prior_)
print(classification_report(test_red_y,GNB.predict(X_test)))

param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],'solver':['liblinear', 'newton-cg', 'sag', 'saga' ,'lbfgs']}
LR = GridSearchCV(LogisticRegression(penalty='l2',max_iter=16000), param_grid)
LR.fit(X_train,train_red_y)
print("Score:",LR.score(X_test,test_red_y))
print("Best Parameters:",LR.best_params_," Best Score:",LR.best_score_)
print(classification_report(test_red_y,LR.predict(X_test)))

param_grid = {"C": list(range(1,15)),"kernel" : ['linear', 'rbf', 'poly']}
SVM = GridSearchCV(SVC(),param_grid=param_grid)
SVM.fit(X_train,train_red_y)
print("Score:",SVM.score(X_test,test_red_y))
print("Best Parameters:",SVM.best_params_," Best Score:",SVM.best_score_)
print(classification_report(test_red_y,SVM.predict(X_test)))

param_grid = {
        'min_child_weight': [1, 5, 10],
        'gamma': [0.5, 1, 1.5, 2, 5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': [3, 4, 5]
        }
XGB = GridSearchCV(XGBClassifier(),param_grid,verbose=1)
XGB.fit(X_train,train_red_y)
print("Score:",accuracy_score(test_red_y,XGB.predict(X_test)))
print("Best Parameters:",XGB.best_params_," Best Score:",XGB.best_score_)
print(classification_report(test_red_y,XGB.predict(X_test)))

param_grid = {'criterion':['gini','entropy'],'max_depth':list(range(1,10)),'max_features':['auto','sqrt','log2'],'warm_start':[True,False]}
RFC = GridSearchCV(RandomForestClassifier(),param_grid,verbose=1)
RFC.fit(X_train,train_red_y)
print("Score:",RFC.score(X_test,test_red_y))
print("Best Parameters:",RFC.best_params_," Best Score:",RFC.best_score_)
print(classification_report(test_red_y,RFC.predict(X_test)))

"""### Sub-Classification (Bad,Medium,Good)"""

train_red_X, test_red_X, train_red_y, test_red_y = train_test_split(df_new_red.drop(["rating"],axis=1,inplace=False),df_new_red["rating"],test_size=0.25,random_state=42)

scalar=StandardScaler()
X_train=scalar.fit_transform(train_red_X,train_red_y)
X_test=scalar.fit_transform(test_red_X,test_red_y)

GNB = GaussianNB()
GNB.fit(X_train,train_red_y)
print("Score:",GNB.score(X_test,test_red_y))
print("Class priors:",GNB.class_prior_)
print(classification_report(test_red_y,GNB.predict(X_test)))

param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],'solver':['liblinear', 'newton-cg', 'sag', 'saga' ,'lbfgs']}
LR = GridSearchCV(LogisticRegression(penalty='l2',max_iter=16000), param_grid)
LR.fit(X_train,train_red_y)
print("Score:",LR.score(X_test,test_red_y))
print("Best Parameters:",LR.best_params_," Best Score:",LR.best_score_)
print(classification_report(test_red_y,LR.predict(X_test)))

param_grid = {"C": list(range(1,15)),"kernel" : ['linear', 'rbf', 'poly']}
SVM = GridSearchCV(SVC(),param_grid=param_grid)
SVM.fit(X_train,train_red_y)
print("Score:",SVM.score(X_test,test_red_y))
print("Best Parameters:",SVM.best_params_," Best Score:",SVM.best_score_)
print(classification_report(test_red_y,SVM.predict(X_test)))

param_grid = {
        'min_child_weight': [1, 5, 10],
        'gamma': [0.5, 1, 1.5, 2, 5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': [3, 4, 5]
        }
XGB = GridSearchCV(XGBClassifier(),param_grid,verbose=1)
XGB.fit(X_train,train_red_y)
print("Score:",accuracy_score(test_red_y,XGB.predict(X_test)))
print("Best Parameters:",XGB.best_params_," Best Score:",XGB.best_score_)
print(classification_report(test_red_y,XGB.predict(X_test)))

param_grid = {'criterion':['gini','entropy'],'max_depth':list(range(1,10)),'max_features':['auto','sqrt','log2'],'warm_start':[True,False]}
RFC = GridSearchCV(RandomForestClassifier(),param_grid,verbose=1,n_jobs=5)
RFC.fit(X_train,train_red_y)
print("Score:",RFC.score(X_test,test_red_y))
print("Best Parameters:",RFC.best_params_," Best Score:",RFC.best_score_)
print(classification_report(test_red_y,RFC.predict(X_test)))

"""### Sub-Classification (Bad,Good)"""

train_red_X, test_red_X, train_red_y, test_red_y = train_test_split(df_bin_red.drop(["rating"],axis=1,inplace=False),df_bin_red["rating"],test_size=0.25,random_state=42)

scalar=StandardScaler()
X_train=scalar.fit_transform(train_red_X,train_red_y)
X_test=scalar.fit_transform(test_red_X,test_red_y)

GNB = GaussianNB()
GNB.fit(X_train,train_red_y)
print("Score:",GNB.score(X_test,test_red_y))
print("Class priors:",GNB.class_prior_)
print(classification_report(test_red_y,GNB.predict(X_test)))

param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],'solver':['liblinear', 'newton-cg', 'sag', 'saga' ,'lbfgs']}
LR = GridSearchCV(LogisticRegression(penalty='l2',max_iter=16000), param_grid)
LR.fit(X_train,train_red_y)
print("Score:",LR.score(X_test,test_red_y))
print("Best Parameters:",LR.best_params_," Best Score:",LR.best_score_)
print(classification_report(test_red_y,LR.predict(X_test)))

param_grid = {"C": list(range(1,15)),"kernel" : ['linear', 'rbf', 'poly']}
SVM = GridSearchCV(SVC(),param_grid=param_grid)
SVM.fit(X_train,train_red_y)
print("Score:",SVM.score(X_test,test_red_y))
print("Best Parameters:",SVM.best_params_," Best Score:",SVM.best_score_)
print(classification_report(test_red_y,SVM.predict(X_test)))

"""This model actually shows a tolerable result.It can be observed that the model shows high precision when predicting the 'bad' class.(From all the predicted 'bad' results,92% are actually true regarding to the class.)

It can be observed that the model also shows high recall when predicting the 'bad' class.

Despite the fact that the created model tends to work good on 'bad' class, model does not show a high performance on 'good' class.Model confuses when trying to predict on the 'good' class but still this is not a very low result.
"""

param_grid = {
        'min_child_weight': [1, 5, 10],
        'gamma': [0.5, 1, 1.5, 2, 5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': [3, 4, 5]
        }
XGB = GridSearchCV(XGBClassifier(),param_grid,verbose=1,n_jobs=5)
XGB.fit(X_train,train_red_y)
print("Score:",accuracy_score(test_red_y,XGB.predict(X_test)))
print("Best Parameters:",XGB.best_params_," Best Score:",XGB.best_score_)
print(classification_report(test_red_y,XGB.predict(X_test)))

param_grid = {'criterion':['gini','entropy'],'max_depth':list(range(1,10)),'max_features':['auto','sqrt','log2'],'warm_start':[True,False]}
RFC = GridSearchCV(RandomForestClassifier(),param_grid,verbose=1,n_jobs=5)
RFC.fit(X_train,train_red_y)
print("Score:",RFC.score(X_test,test_red_y))
print("Best Parameters:",RFC.best_params_," Best Score:",RFC.best_score_)
print(classification_report(test_red_y,RFC.predict(X_test)))

"""## White Wine

### Without Sub-Classification

Train,test splitting using test size = 0.2 and random state as 42
"""

train_white_X, test_white_X, train_white_y, test_white_y = train_test_split(df_white.drop(["quality"],axis=1,inplace=False),df_white["quality"],test_size=0.2,random_state=42)

"""We are using StandardScaler for scaling.Values would range from -1 to 1."""

scalar=StandardScaler()
X_train=scalar.fit_transform(train_white_X,train_white_y)
X_test=scalar.fit_transform(test_white_X,test_white_y)

GNB = GaussianNB()
GNB.fit(X_train,train_white_y)
print("Score:",GNB.score(X_test,test_white_y))
print("Class priors:",GNB.class_prior_)
print(classification_report(test_white_y,GNB.predict(X_test)))

param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],'solver':['liblinear', 'newton-cg', 'sag', 'saga' ,'lbfgs']}
LR = GridSearchCV(LogisticRegression(penalty='l2',max_iter=16000), param_grid)
LR.fit(X_train,train_white_y)
print("Score:",LR.score(X_test,test_white_y))
print("Best Parameters:",LR.best_params_," Best Score:",LR.best_score_)
print(classification_report(test_white_y,LR.predict(X_test)))

param_grid = {"C": list(range(1,15)),"kernel" : ['linear', 'rbf', 'poly']}
SVM = GridSearchCV(SVC(),param_grid=param_grid)
SVM.fit(X_train,train_white_y)
print("Score:",SVM.score(X_test,test_white_y))
print("Best Parameters:",SVM.best_params_," Best Score:",SVM.best_score_)
print(classification_report(test_white_y,SVM.predict(X_test)))

param_grid = {
        'min_child_weight': [1, 5, 10],
        'gamma': [0.5, 1, 1.5, 2, 5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': [3, 4, 5]
        }
XGB = GridSearchCV(XGBClassifier(),param_grid,verbose=1)
XGB.fit(X_train,train_white_y)
print("Score:",accuracy_score(test_white_y,XGB.predict(X_test)))
print("Best Parameters:",XGB.best_params_," Best Score:",XGB.best_score_)
print(classification_report(test_white_y,XGB.predict(X_test)))

param_grid = {'criterion':['gini','entropy'],'max_depth':list(range(1,10)),'max_features':['auto','sqrt','log2'],'warm_start':[True,False]}
RFC = GridSearchCV(RandomForestClassifier(),param_grid,verbose=1,n_jobs=5)
RFC.fit(X_train,train_white_y)
print("Score:",RFC.score(X_test,test_white_y))
print("Best Parameters:",RFC.best_params_," Best Score:",RFC.best_score_)
print(classification_report(test_white_y,RFC.predict(X_test)))

"""### Sub-Classification (Bad,Medium,Good)"""

train_white_X, test_white_X, train_white_y, test_white_y = train_test_split(df_new_white.drop(["rating"],axis=1,inplace=False),df_new_white["rating"],test_size=0.25,random_state=42)

scalar=StandardScaler()
X_train=scalar.fit_transform(train_white_X,train_white_y)
X_test=scalar.fit_transform(test_white_X,test_white_y)

GNB = GaussianNB()
GNB.fit(X_train,train_white_y)
print("Score:",GNB.score(X_test,test_white_y))
print("Class priors:",GNB.class_prior_)
print(classification_report(test_white_y,GNB.predict(X_test)))

param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],'solver':['liblinear', 'newton-cg', 'sag', 'saga' ,'lbfgs']}
LR = GridSearchCV(LogisticRegression(penalty='l2',max_iter=16000), param_grid)
LR.fit(X_train,train_white_y)
print("Score:",LR.score(X_test,test_white_y))
print("Best Parameters:",LR.best_params_," Best Score:",LR.best_score_)
print(classification_report(test_white_y,LR.predict(X_test)))

param_grid = {"C": list(range(1,15)),"kernel" : ['linear', 'rbf', 'poly']}
SVM = GridSearchCV(SVC(),param_grid=param_grid)
SVM.fit(X_train,train_white_y)
print("Score:",SVM.score(X_test,test_white_y))
print("Best Parameters:",SVM.best_params_," Best Score:",SVM.best_score_)
print(classification_report(test_white_y,SVM.predict(X_test)))

param_grid = {
        'min_child_weight': [1, 5, 10],
        'gamma': [0.5, 1, 1.5, 2, 5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': [3, 4, 5]
        }
XGB = GridSearchCV(XGBClassifier(),param_grid,verbose=1,n_jobs=5)
XGB.fit(X_train,train_white_y)
print("Score:",accuracy_score(test_white_y,XGB.predict(X_test)))
print("Best Parameters:",XGB.best_params_," Best Score:",XGB.best_score_)
print(classification_report(test_white_y,XGB.predict(X_test)))

param_grid = {'criterion':['gini','entropy'],'max_depth':list(range(1,10)),'max_features':['auto','sqrt','log2'],'warm_start':[True,False]}
RFC = GridSearchCV(RandomForestClassifier(),param_grid,verbose=1,n_jobs=5)
RFC.fit(X_train,train_white_y)
print("Score:",RFC.score(X_test,test_white_y))
print("Best Parameters:",RFC.best_params_," Best Score:",RFC.best_score_)
print(classification_report(test_white_y,RFC.predict(X_test)))

"""### Sub-Classification (Bad,Good)"""

train_white_X, test_white_X, train_white_y, test_white_y = train_test_split(df_bin_white.drop(["rating"],axis=1,inplace=False),df_bin_white["rating"],test_size=0.25,random_state=42)

scalar=StandardScaler()
X_train=scalar.fit_transform(train_white_X,train_white_y)
X_test=scalar.fit_transform(test_white_X,test_white_y)

GNB = GaussianNB()
GNB.fit(X_train,train_white_y)
print("Score:",GNB.score(X_test,test_white_y))
print("Class priors:",GNB.class_prior_)
print(classification_report(test_white_y,GNB.predict(X_test)))

param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],'solver':['liblinear', 'newton-cg', 'sag', 'saga' ,'lbfgs']}
LR = GridSearchCV(LogisticRegression(penalty='l2',max_iter=16000), param_grid)
LR.fit(X_train,train_white_y)
print("Score:",LR.score(X_test,test_white_y))
print("Best Parameters:",LR.best_params_," Best Score:",LR.best_score_)
print(classification_report(test_white_y,LR.predict(X_test)))

param_grid = {"C": list(range(1,15)),"kernel" : ['linear', 'rbf', 'poly']}
SVM = GridSearchCV(SVC(),param_grid=param_grid)
SVM.fit(X_train,train_white_y)
print("Score:",SVM.score(X_test,test_white_y))
print("Best Parameters:",SVM.best_params_," Best Score:",SVM.best_score_)
print(classification_report(test_white_y,SVM.predict(X_test)))

param_grid = {
        'min_child_weight': [1, 5, 10],
        'gamma': [0.5, 1, 1.5, 2, 5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': [3, 4, 5]
        }
XGB = GridSearchCV(XGBClassifier(),param_grid,verbose=1,n_jobs=5)
XGB.fit(X_train,train_white_y)
print("Score:",accuracy_score(test_white_y,XGB.predict(X_test)))
print("Best Parameters:",XGB.best_params_," Best Score:",XGB.best_score_)
print(classification_report(test_white_y,XGB.predict(X_test)))

param_grid = {'criterion':['gini','entropy'],'max_depth':list(range(1,10)),'max_features':['auto','sqrt','log2'],'warm_start':[True,False]}
RFC = GridSearchCV(RandomForestClassifier(),param_grid,verbose=1,n_jobs=5)
RFC.fit(X_train,train_white_y)
print("Score:",RFC.score(X_test,test_white_y))
print("Best Parameters:",RFC.best_params_," Best Score:",RFC.best_score_)
print(classification_report(test_white_y,RFC.predict(X_test)))

"""#Final Analysis

The dataset contains 2 different wine types,red and white respectively.For the red wine dataset,we have 6 quality rankings ranging from 3 to 8.These rankings are not distributed equally on the dataset making the dataset an inbalanced dataset.This causes few problems when models are built for wine classification which can be observed from Without Sub-Classification part.The models built for prediction almost never predicts quality ranks 3,4,8.
This condition is tried to be overcomed by using a sub-classification method making the classes more equally balanced.We sub-categorize quality rankings from (3,8) to 'bad','medium','good'.Quality rankings below 6 is classified as 'bad' while quality rankings equals to 6 are classified as 'medium' and finally quality rankings above 6 classified as 'good'.This process allowed us to easily built models able to show better performances.In the final step we sub-categorize the wines in to 2 categories making them 'bad' when quality rankings below 7 and 'good' when quality rankings above 6.This process allowed us to built models able to show almost 90% accuracy but this condition is a beguiling condition.When we inspect the results of the classification model,we observe models able to show high precision and recall on 'bad' class but low precision and recall on 'good' class.

White wine dataset is not affected by the inbalance issue as much as the red wine dataset.Models that are built on using the white wine dataset actually able to find almost all of the quality rankings(Maybe except quality ranking '9',which is caused due to the fact that really less information is known about that class '9'->Only few instances of quality '9' exists.SMOTE can be used in this issue).
"""